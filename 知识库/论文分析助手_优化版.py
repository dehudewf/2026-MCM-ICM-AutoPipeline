"""
MCM/ICM è®ºæ–‡åˆ†æåŠ©æ‰‹ - ä¼˜åŒ–ç‰ˆ
ç»“åˆè‡ªåŠ¨æå–å’Œäººå·¥å®¡æ ¸ï¼Œæä¾›æ›´å‡†ç¡®çš„åˆ†æç»“æœ
"""

import os
import re
import pandas as pd
from pathlib import Path
from collections import Counter
import json

try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False


# é…ç½®è·¯å¾„
BASE_DIR = Path("MCMICM")
YEARS = {
    "2022": BASE_DIR / "2022ç¾èµ›ä¼˜ç§€è®ºæ–‡é›†" / "2022 ç¾èµ› C",
    "2023": BASE_DIR / "2023" / "C",
    "2024": BASE_DIR / "2024" / "C" / "student paper"
}

def extract_text_from_pdf(pdf_path, max_pages=25):
    """ä»PDFä¸­æå–æ–‡æœ¬ - æ”¹è¿›ç‰ˆ"""
    if not PDF_AVAILABLE:
        return "", 0
    
    try:
        text = ""
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)
            num_pages = min(total_pages, max_pages)
            
            for page_num in range(num_pages):
                try:
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                except:
                    continue
        
        return text, total_pages
    except Exception as e:
        print(f"  âš ï¸ è¯»å–å¤±è´¥: {e}")
        return "", 0

def extract_abstract(text):
    """æå–æ‘˜è¦éƒ¨åˆ†"""
    # å°è¯•æ‰¾åˆ°æ‘˜è¦
    abstract_patterns = [
        r'ABSTRACT\s*\n(.*?)\n(?:Keywords|Introduction|1\.|INTRODUCTION)',
        r'Abstract\s*\n(.*?)\n(?:Keywords|Introduction|1\.|INTRODUCTION)',
        r'Summary\s*\n(.*?)\n(?:Keywords|Introduction|1\.|INTRODUCTION)'
    ]
    
    for pattern in abstract_patterns:
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            abstract = match.group(1).strip()
            # é™åˆ¶é•¿åº¦
            if len(abstract) < 2000:
                return abstract[:500] + "..." if len(abstract) > 500 else abstract
    
    return "æœªæå–åˆ°æ‘˜è¦"

def smart_model_extraction(text):
    """æ™ºèƒ½æ¨¡å‹æå– - æ”¹è¿›ç‰ˆ"""
    # æ‰©å±•çš„æ¨¡å‹å…³é”®è¯åº“ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    model_patterns = {
        # é«˜ä¼˜å…ˆçº§ï¼šå®Œæ•´æ¨¡å‹åç§°
        "priority_high": {
            "Random Forest Regression": "éšæœºæ£®æ—å›å½’",
            "Random Forest": "éšæœºæ£®æ—",
            "XGBoost": "XGBoost",
            "Gradient Boosting": "æ¢¯åº¦æå‡",
            "Neural Network": "ç¥ç»ç½‘ç»œ",
            "Deep Learning": "æ·±åº¦å­¦ä¹ ",
            "LSTM": "LSTM",
            "GRU": "GRU",
            "CNN": "å·ç§¯ç¥ç»ç½‘ç»œ",
            "RNN": "å¾ªç¯ç¥ç»ç½‘ç»œ",
            "Support Vector Machine": "æ”¯æŒå‘é‡æœº",
            "SVM": "SVM",
            "Decision Tree": "å†³ç­–æ ‘",
            "K-Means": "Kå‡å€¼",
            "DBSCAN": "DBSCAN",
            "Hierarchical Clustering": "å±‚æ¬¡èšç±»",
            
            # æ—¶é—´åºåˆ—
            "ARIMA": "ARIMA",
            "SARIMA": "SARIMA",
            "Exponential Smoothing": "æŒ‡æ•°å¹³æ»‘",
            "Holt-Winters": "Holt-Winters",
            
            # ç»Ÿè®¡æ¨¡å‹
            "Linear Regression": "çº¿æ€§å›å½’",
            "Logistic Regression": "é€»è¾‘å›å½’",
            "Polynomial Regression": "å¤šé¡¹å¼å›å½’",
            "Ridge Regression": "å²­å›å½’",
            "Lasso Regression": "Lassoå›å½’",
            "Bayesian Network": "è´å¶æ–¯ç½‘ç»œ",
            "Naive Bayes": "æœ´ç´ è´å¶æ–¯",
            
            # ä¼˜åŒ–ç®—æ³•
            "Linear Programming": "çº¿æ€§è§„åˆ’",
            "Integer Programming": "æ•´æ•°è§„åˆ’",
            "Mixed Integer": "æ··åˆæ•´æ•°è§„åˆ’",
            "Genetic Algorithm": "é—ä¼ ç®—æ³•",
            "Simulated Annealing": "æ¨¡æ‹Ÿé€€ç«",
            "Particle Swarm Optimization": "ç²’å­ç¾¤ä¼˜åŒ–",
            "PSO": "ç²’å­ç¾¤ä¼˜åŒ–",
            "Ant Colony": "èšç¾¤ç®—æ³•",
            
            # å…¶ä»–
            "Monte Carlo Simulation": "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ",
            "Markov Chain": "é©¬å°”å¯å¤«é“¾",
            "Hidden Markov Model": "éšé©¬å°”å¯å¤«æ¨¡å‹",
            "HMM": "éšé©¬å°”å¯å¤«",
            "Principal Component Analysis": "ä¸»æˆåˆ†åˆ†æ",
            "PCA": "ä¸»æˆåˆ†åˆ†æ",
            "Factor Analysis": "å› å­åˆ†æ",
            "Graph Theory": "å›¾è®º",
            "PageRank": "PageRank",
            "Dijkstra": "Dijkstraç®—æ³•",
            "A* Algorithm": "A*ç®—æ³•"
        },
        
        # ä¸­ä¼˜å…ˆçº§ï¼šé€šç”¨æœ¯è¯­ï¼ˆéœ€è¦ä¸Šä¸‹æ–‡éªŒè¯ï¼‰
        "priority_medium": {
            "Regression Model": "å›å½’æ¨¡å‹",
            "Classification Model": "åˆ†ç±»æ¨¡å‹",
            "Clustering": "èšç±»",
            "Time Series": "æ—¶é—´åºåˆ—",
            "Optimization": "ä¼˜åŒ–æ¨¡å‹",
            "Network Analysis": "ç½‘ç»œåˆ†æ"
        }
    }
    
    found_models = {}
    text_upper = text.upper()
    
    # å…ˆåŒ¹é…é«˜ä¼˜å…ˆçº§
    for eng_name, cn_name in model_patterns["priority_high"].items():
        # ä½¿ç”¨è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
        pattern = r'\b' + re.escape(eng_name.upper()) + r'\b'
        matches = re.findall(pattern, text_upper)
        if matches:
            count = len(matches)
            # åªä¿ç•™å‡ºç°2æ¬¡ä»¥ä¸Šçš„ï¼ˆæ›´å¯èƒ½æ˜¯çœŸæ­£ä½¿ç”¨çš„æ¨¡å‹ï¼‰
            if count >= 2:
                found_models[cn_name] = count
    
    # æŒ‰å‡ºç°é¢‘ç‡æ’åº
    sorted_models = sorted(found_models.items(), key=lambda x: x[1], reverse=True)
    
    # è¿”å›å‰5ä¸ªæœ€å¸¸å‡ºç°çš„æ¨¡å‹
    top_models = [model for model, count in sorted_models[:5]]
    
    return top_models, found_models

def extract_sections(text):
    """æå–è®ºæ–‡ç« èŠ‚ç»“æ„"""
    sections_found = []
    
    # å¸¸è§ç« èŠ‚æ ‡é¢˜
    section_patterns = [
        (r'\b(ABSTRACT|Abstract)\b', 'æ‘˜è¦'),
        (r'\b(INTRODUCTION|Introduction|1\.?\s*Introduction)\b', 'å¼•è¨€'),
        (r'\b(PROBLEM\s*ANALYSIS|Problem\s*Analysis)\b', 'é—®é¢˜åˆ†æ'),
        (r'\b(ASSUMPTIONS|Assumptions)\b', 'å‡è®¾'),
        (r'\b(MODEL|Model|MODELING|Modeling)\b', 'æ¨¡å‹'),
        (r'\b(ALGORITHM|Algorithm)\b', 'ç®—æ³•'),
        (r'\b(DATA|Data)\b', 'æ•°æ®'),
        (r'\b(RESULTS|Results)\b', 'ç»“æœ'),
        (r'\b(VALIDATION|Validation)\b', 'éªŒè¯'),
        (r'\b(SENSITIVITY|Sensitivity)\b', 'æ•æ„Ÿæ€§åˆ†æ'),
        (r'\b(CONCLUSION|Conclusion)\b', 'ç»“è®º'),
        (r'\b(REFERENCES|References)\b', 'å‚è€ƒæ–‡çŒ®')
    ]
    
    for pattern, name in section_patterns:
        if re.search(pattern, text):
            sections_found.append(name)
    
    return sections_found

def count_figures_tables_improved(text):
    """æ”¹è¿›çš„å›¾è¡¨ç»Ÿè®¡"""
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼
    figures = set(re.findall(r'Figure\s+(\d+)', text, re.IGNORECASE))
    tables = set(re.findall(r'Table\s+(\d+)', text, re.IGNORECASE))
    
    return len(figures), len(tables)

def analyze_paper_smart(pdf_path):
    """æ™ºèƒ½åˆ†æå•ç¯‡è®ºæ–‡"""
    print(f"\nğŸ“„ åˆ†æ: {pdf_path.name}")
    
    # æå–æ–‡æœ¬
    text, total_pages = extract_text_from_pdf(pdf_path)
    
    if not text or len(text) < 100:
        return {
            "çŠ¶æ€": "âŒ æå–å¤±è´¥",
            "é¡µæ•°": total_pages,
            "æ‘˜è¦": "",
            "æ¨¡å‹": [],
            "æ¨¡å‹è¯¦æƒ…": {},
            "ç« èŠ‚": [],
            "å›¾æ•°": 0,
            "è¡¨æ•°": 0
        }
    
    # æå–æ‘˜è¦
    abstract = extract_abstract(text)
    
    # æ™ºèƒ½æå–æ¨¡å‹
    models, model_details = smart_model_extraction(text)
    
    # æå–ç« èŠ‚
    sections = extract_sections(text)
    
    # ç»Ÿè®¡å›¾è¡¨
    fig_count, table_count = count_figures_tables_improved(text)
    
    print(f"  âœ“ é¡µæ•°: {total_pages}")
    print(f"  âœ“ è¯†åˆ«æ¨¡å‹: {len(models)}ä¸ª")
    print(f"  âœ“ å›¾è¡¨: {fig_count}å›¾ + {table_count}è¡¨")
    
    return {
        "çŠ¶æ€": "âœ“ å®Œæˆ",
        "é¡µæ•°": total_pages,
        "æ‘˜è¦": abstract,
        "æ¨¡å‹": models,
        "æ¨¡å‹è¯¦æƒ…": model_details,
        "ç« èŠ‚": sections,
        "å›¾æ•°": fig_count,
        "è¡¨æ•°": table_count
    }

def batch_analyze():
    """æ‰¹é‡åˆ†ææ‰€æœ‰è®ºæ–‡"""
    if not PDF_AVAILABLE:
        print("\nâŒ é”™è¯¯: éœ€è¦å®‰è£… PyPDF2")
        print("è¿è¡Œ: pip install PyPDF2")
        return None
    
    print("="*70)
    print("ğŸ“Š MCM/ICM Cé¢˜è®ºæ–‡æ™ºèƒ½åˆ†æç³»ç»Ÿ - ä¼˜åŒ–ç‰ˆ")
    print("="*70)
    
    all_results = []
    
    for year, path in YEARS.items():
        if not path.exists():
            print(f"\nâš ï¸  {year}å¹´è·¯å¾„ä¸å­˜åœ¨: {path}")
            continue
        
        pdf_files = list(path.glob("*.pdf"))
        print(f"\n{'='*70}")
        print(f"ğŸ“ {year}å¹´ - å…±{len(pdf_files)}ç¯‡è®ºæ–‡")
        print(f"{'='*70}")
        
        for i, pdf_file in enumerate(pdf_files, 1):
            print(f"\n[{i}/{len(pdf_files)}]", end=" ")
            
            analysis = analyze_paper_smart(pdf_file)
            
            result = {
                "å¹´ä»½": year,
                "è®ºæ–‡ç¼–å·": pdf_file.stem,
                "çŠ¶æ€": analysis["çŠ¶æ€"],
                "é¡µæ•°": analysis["é¡µæ•°"],
                "æ‘˜è¦é¢„è§ˆ": analysis["æ‘˜è¦"][:100] + "..." if len(analysis["æ‘˜è¦"]) > 100 else analysis["æ‘˜è¦"],
                "è¯†åˆ«çš„æ¨¡å‹": ", ".join(analysis["æ¨¡å‹"]) if analysis["æ¨¡å‹"] else "æœªè¯†åˆ«",
                "æ¨¡å‹æ•°é‡": len(analysis["æ¨¡å‹"]),
                "ç« èŠ‚ç»“æ„": ", ".join(analysis["ç« èŠ‚"]),
                "å›¾æ•°é‡": analysis["å›¾æ•°"],
                "è¡¨æ•°é‡": analysis["è¡¨æ•°"],
                "å›¾è¡¨æ€»æ•°": analysis["å›¾æ•°"] + analysis["è¡¨æ•°"],
                
                # éœ€äººå·¥è¡¥å……çš„å­—æ®µ
                "æ ¸å¿ƒæ¨¡å‹": "",
                "åˆ›æ–°ç‚¹": "",
                "æ•°æ®æ¥æº": "",
                "éªŒè¯æ–¹æ³•": "",
                "å¯å€Ÿé‰´åº¦": "",
                "è¯„çº§": "",
                "å¤‡æ³¨": ""
            }
            
            all_results.append(result)
    
    # ä¿å­˜ç»“æœ
    df = pd.DataFrame(all_results)
    output_file = "è®ºæ–‡åˆ†æç»“æœ_ä¼˜åŒ–ç‰ˆ.xlsx"
    df.to_excel(output_file, index=False)
    
    print(f"\n{'='*70}")
    print(f"âœ… åˆ†æå®Œæˆï¼å…±å¤„ç† {len(all_results)} ç¯‡è®ºæ–‡")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜: {output_file}")
    print(f"{'='*70}")
    
    return df

def generate_summary_report(df):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    print("\n" + "="*70)
    print("ğŸ“ˆ ç»Ÿè®¡æŠ¥å‘Š")
    print("="*70)
    
    # å¹´ä»½ç»Ÿè®¡
    print("\nã€å¹´ä»½åˆ†å¸ƒã€‘")
    for year in ["2022", "2023", "2024"]:
        count = len(df[df['å¹´ä»½'] == year])
        if count > 0:
            print(f"  {year}å¹´: {count}ç¯‡")
    
    # æ¨¡å‹ç»Ÿè®¡
    print("\nã€é«˜é¢‘æ¨¡å‹ TOP 15ã€‘")
    all_models = []
    for models_str in df['è¯†åˆ«çš„æ¨¡å‹']:
        if models_str and models_str != "æœªè¯†åˆ«":
            models = [m.strip() for m in models_str.split(',')]
            all_models.extend(models)
    
    if all_models:
        model_counter = Counter(all_models)
        for i, (model, count) in enumerate(model_counter.most_common(15), 1):
            print(f"  {i:2d}. {model:20s} - {count}æ¬¡")
    else:
        print("  æœªè¯†åˆ«åˆ°æ¨¡å‹")
    
    # å›¾è¡¨ç»Ÿè®¡
    print("\nã€å›¾è¡¨ä½¿ç”¨æƒ…å†µã€‘")
    total_figs = df['å›¾æ•°é‡'].sum()
    total_tables = df['è¡¨æ•°é‡'].sum()
    avg_figs = df['å›¾æ•°é‡'].mean()
    avg_tables = df['è¡¨æ•°é‡'].mean()
    
    print(f"  æ€»å›¾æ•°: {total_figs}  å¹³å‡: {avg_figs:.1f}å›¾/ç¯‡")
    print(f"  æ€»è¡¨æ•°: {total_tables}  å¹³å‡: {avg_tables:.1f}è¡¨/ç¯‡")
    print(f"  å›¾è¡¨æœ€å¤šçš„è®ºæ–‡: {df.loc[df['å›¾è¡¨æ€»æ•°'].idxmax(), 'è®ºæ–‡ç¼–å·']} ({df['å›¾è¡¨æ€»æ•°'].max()}ä¸ª)")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_file = "åˆ†æç»Ÿè®¡æŠ¥å‘Š.txt"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write("="*70 + "\n")
        f.write("MCM/ICM Cé¢˜è®ºæ–‡åˆ†æç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("="*70 + "\n\n")
        
        f.write("ã€å¹´ä»½åˆ†å¸ƒã€‘\n")
        for year in ["2022", "2023", "2024"]:
            count = len(df[df['å¹´ä»½'] == year])
            if count > 0:
                f.write(f"  {year}å¹´: {count}ç¯‡\n")
        
        f.write("\nã€é«˜é¢‘æ¨¡å‹ã€‘\n")
        if all_models:
            for i, (model, count) in enumerate(model_counter.most_common(20), 1):
                f.write(f"  {i:2d}. {model:25s} - {count}æ¬¡\n")
        
        f.write(f"\nã€å›¾è¡¨ç»Ÿè®¡ã€‘\n")
        f.write(f"  æ€»å›¾æ•°: {total_figs}\n")
        f.write(f"  æ€»è¡¨æ•°: {total_tables}\n")
        f.write(f"  å¹³å‡å›¾æ•°: {avg_figs:.2f}\n")
        f.write(f"  å¹³å‡è¡¨æ•°: {avg_tables:.2f}\n")
        
        f.write(f"\nã€è®ºæ–‡åˆ—è¡¨ã€‘\n")
        for _, row in df.iterrows():
            f.write(f"\n{row['å¹´ä»½']}-{row['è®ºæ–‡ç¼–å·']}\n")
            f.write(f"  æ¨¡å‹: {row['è¯†åˆ«çš„æ¨¡å‹']}\n")
            f.write(f"  å›¾è¡¨: {row['å›¾æ•°é‡']}å›¾ + {row['è¡¨æ•°é‡']}è¡¨\n")
    
    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def create_model_database(df):
    """åˆ›å»ºæ¨¡å‹æ•°æ®åº“"""
    print("\næ­£åœ¨åˆ›å»ºæ¨¡å‹çŸ¥è¯†åº“...")
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹
    model_papers = {}
    for _, row in df.iterrows():
        if row['è¯†åˆ«çš„æ¨¡å‹'] and row['è¯†åˆ«çš„æ¨¡å‹'] != "æœªè¯†åˆ«":
            models = [m.strip() for m in row['è¯†åˆ«çš„æ¨¡å‹'].split(',')]
            for model in models:
                if model not in model_papers:
                    model_papers[model] = []
                model_papers[model].append(f"{row['å¹´ä»½']}-{row['è®ºæ–‡ç¼–å·']}")
    
    # åˆ›å»ºæ•°æ®åº“
    db_data = []
    for model, papers in sorted(model_papers.items(), key=lambda x: len(x[1]), reverse=True):
        db_data.append({
            "æ¨¡å‹åç§°": model,
            "ä½¿ç”¨æ¬¡æ•°": len(papers),
            "ä½¿ç”¨è®ºæ–‡": "; ".join(papers[:10]),  # æœ€å¤šåˆ—10ç¯‡
            "2022å¹´": sum(1 for p in papers if p.startswith("2022")),
            "2023å¹´": sum(1 for p in papers if p.startswith("2023")),
            "2024å¹´": sum(1 for p in papers if p.startswith("2024")),
            "é€‚ç”¨åœºæ™¯": "",  # äººå·¥è¡¥å……
            "éš¾åº¦è¯„ä¼°": "",  # äººå·¥è¡¥å……
            "æ¨èæŒ‡æ•°": ""   # äººå·¥è¡¥å……
        })
    
    db_df = pd.DataFrame(db_data)
    db_file = "Cé¢˜æ¨¡å‹çŸ¥è¯†åº“_ä¼˜åŒ–ç‰ˆ.xlsx"
    db_df.to_excel(db_file, index=False)
    print(f"âœ… æ¨¡å‹çŸ¥è¯†åº“å·²ä¿å­˜: {db_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("ğŸš€ MCM/ICM Cé¢˜è®ºæ–‡æ™ºèƒ½åˆ†æç³»ç»Ÿ - ä¼˜åŒ–ç‰ˆ")
    print("="*70)
    
    if not PDF_AVAILABLE:
        print("\nâŒ éœ€è¦å®‰è£… PyPDF2")
        print("è¿è¡Œ: pip install PyPDF2")
        return
    
    print("\nğŸ“‹ åŠŸèƒ½è¯´æ˜:")
    print("  1. æ™ºèƒ½æå–PDFå†…å®¹ï¼ˆæ”¹è¿›çš„æ–‡æœ¬æå–ï¼‰")
    print("  2. ç²¾å‡†è¯†åˆ«æ¨¡å‹ï¼ˆåŸºäºé¢‘ç‡å’Œä¸Šä¸‹æ–‡ï¼‰")
    print("  3. æå–æ‘˜è¦å’Œç« èŠ‚ç»“æ„")
    print("  4. å‡†ç¡®ç»Ÿè®¡å›¾è¡¨æ•°é‡")
    print("  5. ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š")
    print("  6. åˆ›å»ºæ¨¡å‹çŸ¥è¯†åº“")
    
    print("\nâš ï¸  é‡è¦æç¤º:")
    print("  - è‡ªåŠ¨è¯†åˆ«ç»“æœä»…ä¾›å‚è€ƒ")
    print("  - å»ºè®®äººå·¥å¤æ ¸'æ ¸å¿ƒæ¨¡å‹'ã€'åˆ›æ–°ç‚¹'ç­‰å­—æ®µ")
    print("  - å¯åœ¨Excelä¸­ç›´æ¥ç¼–è¾‘è¡¥å……ä¿¡æ¯")
    
    input("\næŒ‰å›è½¦é”®å¼€å§‹åˆ†æ...")
    
    # æ‰¹é‡åˆ†æ
    df = batch_analyze()
    
    if df is not None and len(df) > 0:
        # ç”ŸæˆæŠ¥å‘Š
        generate_summary_report(df)
        
        # åˆ›å»ºçŸ¥è¯†åº“
        create_model_database(df)
        
        print("\n" + "="*70)
        print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
        print("="*70)
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("  1. è®ºæ–‡åˆ†æç»“æœ_ä¼˜åŒ–ç‰ˆ.xlsx - è¯¦ç»†åˆ†æç»“æœï¼ˆå¯ç¼–è¾‘ï¼‰")
        print("  2. åˆ†æç»Ÿè®¡æŠ¥å‘Š.txt - ç»Ÿè®¡æ±‡æ€»")
        print("  3. Cé¢˜æ¨¡å‹çŸ¥è¯†åº“_ä¼˜åŒ–ç‰ˆ.xlsx - æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡")
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("  1. æ‰“å¼€Excelæ–‡ä»¶ï¼Œäººå·¥å¤æ ¸å’Œè¡¥å……ä¿¡æ¯")
        print("  2. é‡ç‚¹å…³æ³¨'è¯†åˆ«çš„æ¨¡å‹'åˆ—ï¼Œç¡®è®¤æ˜¯å¦å‡†ç¡®")
        print("  3. è¡¥å……'æ ¸å¿ƒæ¨¡å‹'ã€'åˆ›æ–°ç‚¹'ã€'å¯å€Ÿé‰´åº¦'ç­‰å­—æ®µ")
        print("  4. å‚è€ƒç»Ÿè®¡æŠ¥å‘Šï¼Œäº†è§£æ•´ä½“è¶‹åŠ¿")

if __name__ == "__main__":
    main()
