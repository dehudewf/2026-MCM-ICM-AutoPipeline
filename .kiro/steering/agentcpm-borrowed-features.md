# AgentCPM Borrowed Features for MCM Paper Writing

> ä» AgentCPM æ¶æ„ä¸­æå–çš„ä¸¤ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼Œè§£å†³ MCM è®ºæ–‡å†™ä½œçš„å…³é”®ç—›ç‚¹

---

## ğŸ¥‡ P0: Jinja2 Template System (é˜²æ­¢æ•°æ®å¹»è§‰)

### é—®é¢˜èƒŒæ™¯

| é—®é¢˜ | åæœ | ä¸¥é‡ç¨‹åº¦ |
|-----|------|---------|
| AI ç¼–é€ æ•°æ® | "USA: 92Â±5 medals" â† å®Œå…¨è™šæ„ | ğŸ”´ è‡´å‘½ |
| æ— æ³•æ³¨å…¥çœŸå®ç»“æœ | è®ºæ–‡æ•°æ®ä¸æ¨¡å‹ç»“æœä¸ä¸€è‡´ | ğŸ”´ è‡´å‘½ |
| æ¨¡æ¿ä¸æ•°æ®è€¦åˆ | ä¿®æ”¹å›°éš¾ï¼Œå¤ç”¨æ€§å·® | ğŸŸ¡ ä¸­ç­‰ |

### è§£å†³æ–¹æ¡ˆï¼šJinja2 åŠ¨æ€æ¨¡æ¿æ³¨å…¥

```yaml
æ ¸å¿ƒåŸç†:
  1. æ¨¡æ¿å®šä¹‰ç»“æ„ (template.jinja)
  2. æ•°æ®ç‹¬ç«‹å­˜å‚¨ (model_results.json)
  3. è¿è¡Œæ—¶æ³¨å…¥ (render)
  4. AI åªèƒ½ä½¿ç”¨æ³¨å…¥çš„æ•°æ®ï¼Œæ— æ³•ç¼–é€ 

æ–‡ä»¶ç»“æ„:
  templates/
    â”œâ”€â”€ mcm_abstract.jinja      # Abstract æ¨¡æ¿
    â”œâ”€â”€ mcm_assumptions.jinja   # Assumptions æ¨¡æ¿
    â”œâ”€â”€ mcm_model_dev.jinja     # Model Development æ¨¡æ¿
    â””â”€â”€ mcm_sensitivity.jinja   # Sensitivity Analysis æ¨¡æ¿
  
  data/
    â”œâ”€â”€ model_results.json      # ä½ çš„çœŸå®æ¨¡å‹ç»“æœ
    â””â”€â”€ assumptions.json        # ä½ çš„å‡è®¾åˆ—è¡¨
```

### Template Schema

#### `mcm_abstract.jinja`

```jinja
{# ================================================================
   MCM Abstract Template (O-Award Structure)
   Variables:
     - problem_description: str
     - model_results: dict
     - available_data: str
   ================================================================ #}

You are an MCM/ICM O-Award paper writing expert.

## Context
**Problem Description:**
{{ problem_description }}

**Your Model Results (USE ONLY THESE - NO FABRICATION):**
{% for task, result in model_results.items() %}
### {{ task }}
{% for key, value in result.items() %}
- {{ key }}: {{ value }}
{% endfor %}
{% endfor %}

**Available Data:**
{{ available_data }}

## Structure Requirements (MANDATORY)

| Section | Format | Example |
|---------|--------|---------|
| Background | 1 sentence: "As [trend] intensifies, [challenge] becomes pressing" | As climate uncertainty increases... |
| Problem Statement | 1 sentence: "In order to [goal], we build [N] models..." | In order to predict medal counts... |
| Task 1 | "For Task 1, we developed [Model] to [purpose]. The result indicates [QUANTITATIVE from model_results]" | For Task 1, we developed ARIMA... |
| Task 2 | "For Task 2, we established [Model]. Applied to [location], results indicate..." | For Task 2, we established HAI... |
| Task 3 | "For Task 3, we applied [Model] to [case]. Score = [VALUE from model_results]" | For Task 3, we applied... |
| Sensitivity | "Finally, we analyze sensitivity and robustness. The model is stable and [adj]" | Finally, we analyze... |
| Keywords | 5-7 terms: [Model1], [Model2], [Method1], [Method2], [Domain] | ARIMA, XGBoost, AHP... |

## Critical Rules
1. ALL numerical results MUST come from `model_results` above
2. DO NOT fabricate any numbers - use ONLY provided results
3. Use "For Task N, we..." format EXACTLY
4. Word count: 250-350 words

## Output Format
<thought>
- Which results from model_results will I use?
- How will I ensure O-Award structure compliance?
</thought>

<action>
[Your Abstract in Markdown]
</action>
```

#### `mcm_assumptions.jinja`

```jinja
{# ================================================================
   MCM Assumptions Template
   Variables:
     - assumptions_list: list[dict] with keys: content, justification
   ================================================================ #}

You are an MCM/ICM O-Award paper writing expert.

## Provided Assumptions (USE EXACTLY AS GIVEN)
{% for i, assumption in enumerate(assumptions_list, 1) %}
**Assumption {{ i }}:** {{ assumption.content }}
**Justification:** {{ assumption.justification }}
{% endfor %}

## O-Award Format Requirements

| Component | Requirement | Frequency in O-Award Papers |
|-----------|-------------|----------------------------|
| Assumption N: | Numbered format required | 29/20 papers |
| Justification N: | MUST follow each assumption | 43 occurrences |
| Total count | 3-5 assumptions | Standard |
| Intro sentence | Optional but recommended | 8/20 papers |

## Output Format

```
2 Assumptions and Justifications

[Optional intro: "It is not possible to model every possible scenario. 
So we make some reasonable assumptions to simplify the model, each with 
a corresponding explanation:"]

Assumption 1: [content]
Justification: [justification]

Assumption 2: [content]
Justification: [justification]

[... continue for all assumptions ...]
```

<action>
[Your Assumptions section in Markdown]
</action>
```

### Python Implementation

```python
from jinja2 import Template, Environment, FileSystemLoader
import json

class MCMTemplateEngine:
    """Jinja2-based template engine for MCM paper writing"""
    
    def __init__(self, template_dir: str = "templates"):
        self.env = Environment(loader=FileSystemLoader(template_dir))
    
    def render(self, template_name: str, **context) -> str:
        """
        Render a template with given context
        
        Args:
            template_name: e.g., "mcm_abstract.jinja"
            **context: Variables to inject (problem_description, model_results, etc.)
        
        Returns:
            Rendered prompt string
        """
        template = self.env.get_template(template_name)
        return template.render(**context)
    
    def render_abstract(self, problem_desc: str, model_results: dict) -> str:
        """Convenience method for Abstract generation"""
        return self.render(
            "mcm_abstract.jinja",
            problem_description=problem_desc,
            model_results=model_results,
            available_data="Olympic medal data 2000-2024, GDP, population"
        )

# Usage Example
engine = MCMTemplateEngine()

# Your REAL model results (from Cé¢˜/output/)
model_results = {
    "Task 1": {
        "USA_medals": "113 (95% CI: 105-121)",
        "R_squared": 0.8377,
        "RMSE": 4.23
    },
    "Task 2": {
        "Host_Advantage_Index": "+18.7%",
        "AHP_consistency": 0.06
    },
    "Task 3": {
        "Swimming_ROI": "12.4%",
        "Athletics_ROI": "9.1%"
    }
}

prompt = engine.render_abstract(
    problem_desc="2024 MCM Problem C: Olympic Medal Prediction",
    model_results=model_results
)

# Now call LLM with this prompt - AI cannot fabricate numbers!
```

---

## ğŸ¥‰ P2: Task State Management (è¿­ä»£å¼æ”¹è¿›)

### é—®é¢˜èƒŒæ™¯

| é—®é¢˜ | åæœ | ä¸¥é‡ç¨‹åº¦ |
|-----|------|---------|
| å•æ¬¡ç”Ÿæˆè´¨é‡ä¸ç¨³å®š | ç»“æ„ä¸ç¬¦åˆ O-Award è¦æ±‚ | ğŸŸ¡ ä¸­ç­‰ |
| æ— æ³•è‡ªåŠ¨éªŒè¯è¾“å‡º | éœ€è¦äººå·¥æ£€æŸ¥æ¯ä¸ªç»†èŠ‚ | ğŸŸ¡ ä¸­ç­‰ |
| æ— ä¸Šä¸‹æ–‡è®°å¿† | é‡æ–°ç”Ÿæˆæ—¶ä¸¢å¤±æ”¹è¿›å†å² | ğŸŸ¡ ä¸­ç­‰ |

### è§£å†³æ–¹æ¡ˆï¼šMCMSection with Iterative Refinement

```yaml
æ ¸å¿ƒåŸç†:
  1. æ¯ä¸ªç« èŠ‚ = 1 ä¸ª Task å¯¹è±¡
  2. å†…ç½®éªŒè¯è§„åˆ™ (validation_rules)
  3. è‡ªåŠ¨è¿­ä»£æ”¹è¿› (max_iterations)
  4. ä¿ç•™å¯¹è¯å†å² (conversation_history)

å·¥ä½œæµç¨‹:
  Generate Draft â†’ Validate â†’ [Pass] â†’ Return
                      â†“ [Fail]
             Add Critique â†’ Regenerate â†’ Loop (max 3x)
```

### MCMSection Schema

```python
from typing import Dict, List, Callable, Optional
import re

class MCMSection:
    """
    Represents one paper section with built-in validation and iterative refinement
    
    Attributes:
        name: Section name (e.g., "Abstract", "Assumptions")
        template_path: Path to Jinja2 template
        validation_rules: Dict of rule_name -> check_function
        conversation_history: Multi-turn memory for refinement
        max_iterations: Maximum refinement attempts
    """
    
    def __init__(
        self,
        name: str,
        template_path: str,
        validation_rules: Dict[str, Callable[[str], bool]]
    ):
        self.name = name
        self.template_path = template_path
        self.validation_rules = validation_rules
        self.conversation_history: List[Dict] = []
        self.max_iterations = 3
        self.generation_count = 0
    
    def generate(self, llm_client, context: Dict) -> str:
        """
        Generate section with automatic validation and refinement
        
        Args:
            llm_client: LLM client with create_completion method
            context: Template variables (problem_description, model_results, etc.)
        
        Returns:
            Validated section content
        """
        # Load and render template
        prompt = self._render_template(context)
        
        for iteration in range(self.max_iterations):
            self.generation_count += 1
            
            # Build messages with history
            messages = self.conversation_history + [
                {"role": "user", "content": prompt}
            ]
            
            # Generate
            response = llm_client.create_completion(messages)
            content = self._extract_action(response)
            
            # Validate
            validation = self.validate(content)
            
            if validation["passed"]:
                return content
            
            # Add critique for next iteration
            self.conversation_history.append({"role": "assistant", "content": response})
            self.conversation_history.append({
                "role": "user",
                "content": self._format_critique(validation["errors"])
            })
        
        # Return best attempt after max iterations
        return content
    
    def validate(self, content: str) -> Dict:
        """
        Validate content against all rules
        
        Returns:
            {"passed": bool, "errors": List[str]}
        """
        errors = []
        
        for rule_name, check_fn in self.validation_rules.items():
            try:
                if not check_fn(content):
                    errors.append(f"âŒ Failed: {rule_name}")
            except Exception as e:
                errors.append(f"âš ï¸ Error in {rule_name}: {str(e)}")
        
        return {
            "passed": len(errors) == 0,
            "errors": errors,
            "rules_checked": len(self.validation_rules),
            "rules_passed": len(self.validation_rules) - len(errors)
        }
    
    def _extract_action(self, text: str) -> str:
        """Extract content from <action>...</action> tags"""
        match = re.search(r'<action>(.*?)</action>', text, re.DOTALL)
        return match.group(1).strip() if match else text
    
    def _format_critique(self, errors: List[str]) -> str:
        """Format validation errors as critique prompt"""
        return f"""
Your previous output failed validation. Please fix these issues:

{chr(10).join(errors)}

Regenerate the section with these issues addressed.
"""
    
    def _render_template(self, context: Dict) -> str:
        """Render Jinja2 template with context"""
        from jinja2 import Template
        with open(self.template_path, 'r', encoding='utf-8') as f:
            template = Template(f.read())
        return template.render(**context)
```

### Pre-defined Validation Rules

| Section | Rule Name | Check Function | O-Award Requirement |
|---------|-----------|----------------|---------------------|
| **Abstract** | `has_background` | `lambda c: "As " in c or "The growing" in c` | Background sentence |
| | `has_task_1` | `lambda c: "For Task 1" in c` | Task 1 description |
| | `has_task_2` | `lambda c: "For Task 2" in c` | Task 2 description |
| | `has_task_3` | `lambda c: "For Task 3" in c` | Task 3 description |
| | `has_sensitivity` | `lambda c: "sensitivity" in c.lower()` | Sensitivity statement |
| | `has_keywords` | `lambda c: "Keywords" in c` | Keywords list |
| | `word_count` | `lambda c: 250 <= len(c.split()) <= 350` | 250-350 words |
| **Assumptions** | `has_justification` | `lambda c: "Justification" in c` | Each assumption needs justification |
| | `count_3_to_5` | `lambda c: 3 <= c.count("Assumption") <= 5` | 3-5 assumptions |
| | `numbered_format` | `lambda c: "Assumption 1" in c` | Numbered format |
| **Model Dev** | `has_equations` | `lambda c: "$$" in c or "$" in c` | Mathematical formulas |
| | `has_figure_ref` | `lambda c: "Figure" in c` | Figure references |
| | `has_results` | `lambda c: "result" in c.lower()` | Results presented |

### Pre-configured Section Factory

```python
class MCMSectionFactory:
    """Factory for creating pre-configured MCM sections"""
    
    @staticmethod
    def create_abstract(template_path: str = "templates/mcm_abstract.jinja") -> MCMSection:
        return MCMSection(
            name="Abstract",
            template_path=template_path,
            validation_rules={
                "has_background": lambda c: "As " in c or "The growing" in c,
                "has_task_1": lambda c: "For Task 1" in c,
                "has_task_2": lambda c: "For Task 2" in c,
                "has_task_3": lambda c: "For Task 3" in c,
                "has_sensitivity": lambda c: "sensitivity" in c.lower(),
                "has_keywords": lambda c: "Keywords" in c or "keywords" in c,
                "word_count_min": lambda c: len(c.split()) >= 250,
                "word_count_max": lambda c: len(c.split()) <= 350,
            }
        )
    
    @staticmethod
    def create_assumptions(template_path: str = "templates/mcm_assumptions.jinja") -> MCMSection:
        return MCMSection(
            name="Assumptions",
            template_path=template_path,
            validation_rules={
                "has_justification": lambda c: c.count("Justification") >= 3,
                "count_min": lambda c: c.count("Assumption") >= 3,
                "count_max": lambda c: c.count("Assumption") <= 5,
                "numbered_format": lambda c: "Assumption 1" in c,
            }
        )
    
    @staticmethod
    def create_sensitivity(template_path: str = "templates/mcm_sensitivity.jinja") -> MCMSection:
        return MCMSection(
            name="Sensitivity Analysis",
            template_path=template_path,
            validation_rules={
                "has_parameter_test": lambda c: "parameter" in c.lower(),
                "has_robustness": lambda c: "robust" in c.lower() or "stable" in c.lower(),
                "has_quantitative": lambda c: "%" in c or any(char.isdigit() for char in c),
            }
        )
```

---

## Integration with @executor Agent

### æ›´æ–° Agent è°ƒç”¨æµç¨‹

```yaml
# åœ¨ @executor æ‰§è¡Œå†™ä½œä»»åŠ¡æ—¶ä½¿ç”¨

@executor_write_section:
  trigger: éœ€è¦ç”Ÿæˆè®ºæ–‡ç« èŠ‚æ—¶
  
  workflow:
    1. åŠ è½½ MCMSection:
       section = MCMSectionFactory.create_abstract()
    
    2. å‡†å¤‡çœŸå®æ•°æ®:
       model_results = load_from("Cé¢˜/output/predictions_2028.csv")
    
    3. æ¸²æŸ“æ¨¡æ¿:
       prompt = template_engine.render("mcm_abstract.jinja", model_results=model_results)
    
    4. è¿­ä»£ç”Ÿæˆ:
       content = section.generate(llm_client, context)
    
    5. éªŒè¯é€šè¿‡åè¾“å‡º:
       ```json:a2a:executor_to_redcell
       {
         "section_name": "Abstract",
         "content": content,
         "validation_results": section.validate(content),
         "iterations_used": section.generation_count,
         "data_source": "Cé¢˜/output/predictions_2028.csv"
       }
       ```

  benefits:
    - âœ… æ•°æ®ä¸ä¼šè¢«ç¼–é€ ï¼ˆæ¨¡æ¿æ³¨å…¥ï¼‰
    - âœ… ç»“æ„è‡ªåŠ¨éªŒè¯
    - âœ… è¿­ä»£æ”¹è¿›ç›´åˆ°é€šè¿‡
    - âœ… å¯è¿½æº¯çš„ç”Ÿæˆå†å²
```

### Quick Commands Update

```
# æ–°å¢å‘½ä»¤
@executor å†™ä½œ{ç« èŠ‚} --template={æ¨¡æ¿} --data={æ•°æ®æ–‡ä»¶}
@executor éªŒè¯{ç« èŠ‚} --rules={è§„åˆ™é›†}
@executor è¿­ä»£æ”¹è¿›{ç« èŠ‚} --max={æ¬¡æ•°}
```

---

## File Checklist

| æ–‡ä»¶è·¯å¾„ | ç”¨é€” | çŠ¶æ€ |
|---------|------|------|
| `templates/mcm_abstract.jinja` | Abstract æ¨¡æ¿ | éœ€åˆ›å»º |
| `templates/mcm_assumptions.jinja` | Assumptions æ¨¡æ¿ | éœ€åˆ›å»º |
| `templates/mcm_model_dev.jinja` | Model Development æ¨¡æ¿ | éœ€åˆ›å»º |
| `templates/mcm_sensitivity.jinja` | Sensitivity æ¨¡æ¿ | éœ€åˆ›å»º |
| `src/template_engine.py` | Jinja2 å¼•æ“å°è£… | éœ€åˆ›å»º |
| `src/mcm_section.py` | MCMSection ç±» | éœ€åˆ›å»º |
| `data/model_results.json` | æ¨¡å‹ç»“æœæ•°æ® | ä» Cé¢˜/output/ å¯¼å‡º |

---

## Summary Table

| Feature | Problem Solved | Implementation | Effort | Value |
|---------|---------------|----------------|--------|-------|
| **Jinja2 Templates** | æ•°æ®å¹»è§‰ | æ¨¡æ¿æ³¨å…¥çœŸå®æ•°æ® | 1h | ğŸ”¥ğŸ”¥ğŸ”¥ |
| **MCMSection** | ç»“æ„ä¸åˆè§„ | è‡ªåŠ¨éªŒè¯+è¿­ä»£ | 1h | ğŸ”¥ğŸ”¥ |
| **Validation Rules** | äººå·¥æ£€æŸ¥æˆæœ¬ | é¢„å®šä¹‰è§„åˆ™é›† | 30min | ğŸ”¥ğŸ”¥ |
| **Factory Pattern** | é…ç½®ç¹ç | é¢„é…ç½®ç« èŠ‚å·¥å‚ | 15min | ğŸ”¥ |
