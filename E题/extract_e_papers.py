#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Eé¢˜Oå¥–è®ºæ–‡PDFæå–ä¸åˆ†æè„šæœ¬
é€ç¯‡æå–Abstractå’Œå„ç« èŠ‚å†…å®¹
"""

import os
import re
import pdfplumber
from pathlib import Path

# è·¯å¾„è®¾ç½®
BASE_DIR = Path("/Users/xiaohuiwei/Downloads/è‚–æƒ å¨ç¾èµ›/Eé¢˜/MCMICM Eé¢˜")
OUTPUT_DIR = Path("/Users/xiaohuiwei/Downloads/è‚–æƒ å¨ç¾èµ›/Eé¢˜")

# ä¼˜å…ˆåˆ†æ2024å¹´Eé¢˜Oå¥–è®ºæ–‡
PAPER_DIRS = [
    BASE_DIR / "2024ç¾èµ›Eé¢˜Oå¥–è®ºæ–‡",
    BASE_DIR / "2023å¹´ç¾èµ›Oå¥–è®ºæ–‡" / "E",
]


def extract_text_from_pdf(pdf_path, max_pages=10):
    """æå–PDFå‰Né¡µçš„æ–‡æœ¬"""
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for i, page in enumerate(pdf.pages[:max_pages]):
                page_text = page.extract_text()
                if page_text:
                    text += f"\n{'='*60}\n[Page {i+1}]\n{'='*60}\n"
                    text += page_text
    except Exception as e:
        text = f"Error reading PDF: {e}"
    return text


def extract_sections(text):
    """å°è¯•æå–è®ºæ–‡çš„å„ä¸ªç« èŠ‚"""
    sections = {}
    
    # å¸¸è§ç« èŠ‚æ ‡é¢˜æ¨¡å¼
    section_patterns = [
        (r'(?i)(?:^|\n)\s*(abstract|summary)\s*(?:\n|:)', 'Abstract'),
        (r'(?i)(?:^|\n)\s*(?:\d+\.?\s*)?(introduction)\s*(?:\n|:)', 'Introduction'),
        (r'(?i)(?:^|\n)\s*(?:\d+\.?\s*)?(assumptions?(?:\s+and\s+notations?)?)\s*(?:\n|:)', 'Assumptions'),
        (r'(?i)(?:^|\n)\s*(?:\d+\.?\s*)?((?:data|problem)\s*(?:collection|analysis|description)?)\s*(?:\n|:)', 'Data'),
        (r'(?i)(?:^|\n)\s*(?:\d+\.?\s*)?(model(?:ing)?(?:\s+development)?)\s*(?:\n|:)', 'Model'),
        (r'(?i)(?:^|\n)\s*(?:\d+\.?\s*)?(sensitivity|robustness)\s*(?:\n|:)', 'Sensitivity'),
        (r'(?i)(?:^|\n)\s*(?:\d+\.?\s*)?(strength|weakness|limitation)\s*(?:\n|:)', 'Strengths'),
        (r'(?i)(?:^|\n)\s*(?:\d+\.?\s*)?(conclusion)\s*(?:\n|:)', 'Conclusion'),
    ]
    
    # æŸ¥æ‰¾æ¯ä¸ªç« èŠ‚çš„ä½ç½®
    for pattern, section_name in section_patterns:
        match = re.search(pattern, text)
        if match:
            sections[section_name] = match.start()
    
    return sections


def analyze_paper(pdf_path):
    """åˆ†æå•ç¯‡è®ºæ–‡"""
    paper_id = pdf_path.stem.split('-')[0].split('ã€')[0]
    print(f"\n{'#'*70}")
    print(f"# è®ºæ–‡: {paper_id}")
    print(f"# æ–‡ä»¶: {pdf_path.name}")
    print(f"{'#'*70}")
    
    # æå–æ–‡æœ¬
    text = extract_text_from_pdf(pdf_path, max_pages=8)
    
    if "Error" in text:
        print(f"âŒ {text}")
        return None
    
    # è¾“å‡ºå‰å‡ é¡µå†…å®¹ç”¨äºåˆ†æ
    print(text[:15000])  # æ‰“å°å‰15000å­—ç¬¦ï¼Œè¦†ç›–Abstractå’ŒIntroduction
    
    return {
        'paper_id': paper_id,
        'text': text,
        'sections': extract_sections(text)
    }


def main():
    """ä¸»å‡½æ•°ï¼šæå–å¹¶åˆ†æEé¢˜è®ºæ–‡"""
    print("="*70)
    print("Eé¢˜Oå¥–è®ºæ–‡æå–ä¸åˆ†æ")
    print("="*70)
    
    all_papers = []
    
    # éå†è®ºæ–‡ç›®å½•
    for paper_dir in PAPER_DIRS:
        if not paper_dir.exists():
            print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {paper_dir}")
            continue
            
        print(f"\nğŸ“ æ‰«æç›®å½•: {paper_dir.name}")
        
        pdf_files = sorted(paper_dir.glob("*.pdf"))
        print(f"   æ‰¾åˆ° {len(pdf_files)} ç¯‡è®ºæ–‡")
        
        # åªåˆ†æå‰2ç¯‡ä½œä¸ºç¤ºä¾‹
        for pdf_path in pdf_files[:2]:
            result = analyze_paper(pdf_path)
            if result:
                all_papers.append(result)
    
    print(f"\n{'='*70}")
    print(f"âœ… å…±åˆ†æ {len(all_papers)} ç¯‡è®ºæ–‡")
    print(f"{'='*70}")
    
    return all_papers


if __name__ == "__main__":
    main()
